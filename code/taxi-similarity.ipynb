{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File to do Porto Taxi Trajectory Similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Open the code/global_variables.py file, [or just click here](global_variables.py). And edit the values to fit the given experiment, the name of the chosen subset (\"subset-*size*) and the size of the subset. As well as the coordinates of the geographical area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Make sure you have the needed files/folders for the chosen subset.\n",
    " - in data/raw_data there must be a .csv file with the subset of the chosen size. If not, it must be uploaded.\n",
    " - in data/chosen_data there must be a folder with the same name as global_variables.CHOSEN_SUBSET_NAME. If not, create this empty folder.\n",
    " - in data/hashed_data there must be a folder with the same name as global_variables.CHOSEN_SUBSET_NAME. If not, create this empty folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Run the cells in [code/porto-data.ipynb](porto-data.ipynb), or just run the cell below.\n",
    "This will load the data from the chosen subset into the folder data/chosen_data/subset-'size', each row in the dataset is written in its own text file. I also creates a META-file which contains the name of all the text files in the subset.\n",
    "\n",
    "(Might requires to install nbformat: \"pip install nbformat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the folder: data/chosen_data/subset-subset-100. Files should have been generated.\n"
     ]
    }
   ],
   "source": [
    "%run \"porto-data.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Run the cells in [code/lsh-grid.ipynb](lsh-grid.ipynb), or just run the cell below. This will represent each of the rows/trajectories as an hash, and create a text file for each hashed trajectory in the folder data/hashed_data/subset-'size', as well as a META file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Average runtime  Minimum runtime  Maximum runtime\n",
      "porto         0.021123         0.021031         0.021241\n",
      "                 Average runtime  Minimum runtime  Maximum runtime\n",
      "porto_naive             4.180326         4.131812          4.22105\n",
      "porto_quadrants         2.111735         1.977048          2.29845\n",
      "porto_kd_tree           2.121689         2.065953          2.19822\n",
      "Check the folder: data/hashed_data/subset-100. Files should have been generated.\n"
     ]
    }
   ],
   "source": [
    "%run \"lsh-grid.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Calculate similarities by running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check ../code/experiments/similarities/, it should be a file here named grid_porto-subset-100.csv which contains the similarities in the dataset.\n",
      " Check ../code/experiments/timing/, it should be a file here named similarity_runtimes_grid_porto-{global_variables.CHOSEN_SUBSET_NAME}.csv which contains the time spent to do the hash similarity(?)\n"
     ]
    }
   ],
   "source": [
    "%run similarities-only-grid.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Run the code below to see the clustering of the trajectories. Decide the number of clusters you want by updating number_of_trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to the number of clusters you want (if wanted number is more than 30: update in def plot_clusters() in hierarchical_clustering.py)\n",
    "number_of_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 5 5 0 1 6 8 2 8 2 3 0 2 2 2 9 2 0 5 2 1 5 0 0 2 3 8 0 3 4 5 6 0 2 5 8\n",
      " 4 5 0 1 5 1 5 3 2 9 2 7 0 0 0 1 3 1 0 0 4 2 0 0 0 0 4 4 0 5 1 0 1 8 8 0 0\n",
      " 0 9 0 4 2 0 0 4 0 0 2 0 8 0 9 2 3 0 5 0 0 4 5 4 0 4]\n",
      "TEST123\n",
      "['P_AAAA', 'P_AAAB', 'P_AAAC', 'P_AAAD', 'P_AAAE', 'P_AAAF', 'P_AAAG', 'P_AAAH', 'P_AAAI', 'P_AAAJ', 'P_AAAK', 'P_AAAL', 'P_AAAM', 'P_AAAN', 'P_AAAO', 'P_AAAP', 'P_AAAQ', 'P_AAAR', 'P_AAAS', 'P_AAAT', 'P_AAAU', 'P_AAAV', 'P_AAAW', 'P_AAAX', 'P_AAAY', 'P_AAAZ', 'P_AABA', 'P_AABB', 'P_AABC', 'P_AABD', 'P_AABE', 'P_AABF', 'P_AABG', 'P_AABH', 'P_AABI', 'P_AABJ', 'P_AABK', 'P_AABL', 'P_AABM', 'P_AABN', 'P_AABO', 'P_AABP', 'P_AABQ', 'P_AABR', 'P_AABS', 'P_AABT', 'P_AABU', 'P_AABV', 'P_AABW', 'P_AABX', 'P_AABY', 'P_AABZ', 'P_AACA', 'P_AACB', 'P_AACC', 'P_AACD', 'P_AACE', 'P_AACF', 'P_AACG', 'P_AACH', 'P_AACI', 'P_AACJ', 'P_AACK', 'P_AACL', 'P_AACM', 'P_AACN', 'P_AACO', 'P_AACP', 'P_AACQ', 'P_AACR', 'P_AACS', 'P_AACT', 'P_AACU', 'P_AACV', 'P_AACW', 'P_AACX', 'P_AACY', 'P_AACZ', 'P_AADA', 'P_AADB', 'P_AADC', 'P_AADD', 'P_AADE', 'P_AADF', 'P_AADG', 'P_AADH', 'P_AADI', 'P_AADJ', 'P_AADK', 'P_AADL', 'P_AADM', 'P_AADN', 'P_AADO', 'P_AADP', 'P_AADQ', 'P_AADR', 'P_AADS', 'P_AADT', 'P_AADU', 'P_AADV']\n",
      "{0: ['P_AAAA', 'P_AAAE', 'P_AAAM', 'P_AAAS', 'P_AAAX', 'P_AAAY', 'P_AABC', 'P_AABH', 'P_AABN', 'P_AABX', 'P_AABY', 'P_AABZ', 'P_AACD', 'P_AACE', 'P_AACH', 'P_AACI', 'P_AACJ', 'P_AACK', 'P_AACN', 'P_AACQ', 'P_AACU', 'P_AACV', 'P_AACW', 'P_AACY', 'P_AADB', 'P_AADC', 'P_AADE', 'P_AADF', 'P_AADH', 'P_AADJ', 'P_AADN', 'P_AADP', 'P_AADQ', 'P_AADU'], 1: ['P_AAAF', 'P_AAAV', 'P_AABO', 'P_AABQ', 'P_AACA', 'P_AACC', 'P_AACP', 'P_AACR'], 2: ['P_AAAI', 'P_AAAK', 'P_AAAN', 'P_AAAO', 'P_AAAP', 'P_AAAR', 'P_AAAU', 'P_AAAZ', 'P_AABI', 'P_AABT', 'P_AABV', 'P_AACG', 'P_AADA', 'P_AADG', 'P_AADL'], 3: ['P_AAAL', 'P_AABA', 'P_AABD', 'P_AABS', 'P_AACB', 'P_AADM'], 4: ['P_AAAB', 'P_AABE', 'P_AABL', 'P_AACF', 'P_AACL', 'P_AACM', 'P_AACZ', 'P_AADD', 'P_AADR', 'P_AADT', 'P_AADV'], 5: ['P_AAAC', 'P_AAAD', 'P_AAAT', 'P_AAAW', 'P_AABF', 'P_AABJ', 'P_AABM', 'P_AABP', 'P_AABR', 'P_AACO', 'P_AADO', 'P_AADS'], 6: ['P_AAAG', 'P_AABG'], 7: ['P_AABW'], 8: ['P_AAAH', 'P_AAAJ', 'P_AABB', 'P_AABK', 'P_AACS', 'P_AACT', 'P_AADI'], 9: ['P_AAAQ', 'P_AABU', 'P_AACX', 'P_AADK']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/andrehva/.conda/envs/masteroppgave/lib/python3.12/site-packages/scipy/cluster/hierarchy.py:796: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  return linkage(y, method='ward', metric='euclidean')\n"
     ]
    }
   ],
   "source": [
    "from experiments.hierarchical_clustering import HCA\n",
    "from experiments import davies_bouldin as DB \n",
    "from sklearn import metrics as mcs\n",
    "\n",
    "# Porto Grid similarities\n",
    "#TODO: remove city\n",
    "PortoGrid = HCA(\"Porto\", f\"../code/experiments/similarities/grid_porto-{global_variables.CHOSEN_SUBSET_NAME}.csv\", number_of_clusters )\n",
    "print(PortoGrid.clusters)\n",
    "#PortoGrid.plot_clusters(\"Porto - Grid\")\n",
    "clusters_dict = PortoGrid.get_cluster_dictionary()\n",
    "print(clusters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "['P_AAAE', 'P_AADE', 'P_AAAA', 'P_AAAY', 'P_AAAA', 'P_AABC', 'P_AAAA', 'P_AABZ', 'P_AAAA', 'P_AACD', 'P_AAAA', 'P_AADJ', 'P_AAAX', 'P_AABZ', 'P_AAAX', 'P_AADN', 'P_AAAY', 'P_AACW', 'P_AABC', 'P_AABZ', 'P_AABC', 'P_AADE', 'P_AABC', 'P_AADP', 'P_AABC', 'P_AADU', 'P_AABH', 'P_AADJ', 'P_AABH', 'P_AADQ', 'P_AABN', 'P_AADP', 'P_AABX', 'P_AADU', 'P_AABZ', 'P_AACW', 'P_AABZ', 'P_AADB', 'P_AACD', 'P_AACI', 'P_AACD', 'P_AACN', 'P_AACE', 'P_AADQ', 'P_AACH', 'P_AADQ', 'P_AACI', 'P_AACJ', 'P_AACJ', 'P_AADJ', 'P_AACK', 'P_AADJ', 'P_AACN', 'P_AACW', 'P_AACN', 'P_AADN', 'P_AACW', 'P_AADB', 'P_AADC', 'P_AADN', 'P_AADC', 'P_AADP', 'P_AADF', 'P_AADU', 'P_AADN', 'P_AADP', 'P_AAAE', 'P_AADN', 'P_AAAM', 'P_AACE', 'P_AAAX', 'P_AABY', 'P_AABH', 'P_AABY', 'P_AABH', 'P_AACK', 'P_AABH', 'P_AADF', 'P_AABN', 'P_AACU', 'P_AABN', 'P_AADF', 'P_AABX', 'P_AADB', 'P_AABY', 'P_AACI', 'P_AABY', 'P_AACQ', 'P_AABY', 'P_AADC', 'P_AACE', 'P_AACK', 'P_AACH', 'P_AADH', 'P_AACU', 'P_AADC', 'P_AACY', 'P_AADH', 'P_AADC', 'P_AADF', 'P_AADF', 'P_AADH']\n",
      "6\n",
      "['P_AAAF', 'P_AAAV', 'P_AAAF', 'P_AACR', 'P_AAAV', 'P_AACR']\n",
      "2\n",
      "['P_AACA', 'P_AACP']\n",
      "30\n",
      "['P_AAAK', 'P_AAAP', 'P_AAAI', 'P_AAAR', 'P_AAAI', 'P_AAAU', 'P_AAAI', 'P_AADL', 'P_AAAK', 'P_AADL', 'P_AAAN', 'P_AAAR', 'P_AAAN', 'P_AABT', 'P_AAAN', 'P_AADG', 'P_AAAO', 'P_AABT', 'P_AAAU', 'P_AAAZ', 'P_AACG', 'P_AADL', 'P_AADG', 'P_AADL', 'P_AAAK', 'P_AABT', 'P_AAAK', 'P_AACG', 'P_AAAK', 'P_AADA']\n",
      "2\n",
      "['P_AABL', 'P_AACZ']\n",
      "18\n",
      "['P_AACL', 'P_AADD', 'P_AABE', 'P_AADR', 'P_AAAB', 'P_AACM', 'P_AACM', 'P_AADT', 'P_AADR', 'P_AADT', 'P_AABE', 'P_AADV', 'P_AACL', 'P_AADR', 'P_AACL', 'P_AADV', 'P_AADD', 'P_AADV']\n",
      "4\n",
      "['P_AAAC', 'P_AACO', 'P_AAAD', 'P_AACO']\n",
      "2\n",
      "['P_AABF', 'P_AABP']\n",
      "6\n",
      "['P_AABM', 'P_AADS', 'P_AAAT', 'P_AABR', 'P_AABR', 'P_AADS']\n",
      "2\n",
      "['P_AAAJ', 'P_AACT']\n",
      "4\n",
      "['P_AABK', 'P_AADI', 'P_AACS', 'P_AADI']\n"
     ]
    }
   ],
   "source": [
    "from experiments.frechet_for_taxi_case import find_similarity_in_clusters\n",
    "from experiments.frechet_for_taxi_case import find_similarity_in_cluster\n",
    "from experiments.frechet_for_taxi_case import frechet_similar_taxi_trajectories\n",
    "from experiments.frechet_for_taxi_case import frechet_similar_taxi_and_bus_trajectories\n",
    "from experiments.frechet_for_taxi_case import merge_clusters\n",
    "\n",
    "\n",
    "result = find_similarity_in_clusters(clusters_dict)\n",
    "for r in result:\n",
    "    print(len(r))\n",
    "    print(r)\n",
    "\n",
    "#liste = [['katt', 'hund'], ['hund', 'hest'], ['ku', 'sau'], ['geit', 'hest'], ['ku', 'okse']]\n",
    "#liste2 = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "\n",
    "#liste3 = liste2.copy()\n",
    "#for e in liste2:\n",
    "    #print(e)\n",
    "    #if e in ['a', 'c', 'd']:\n",
    "        #liste3.remove(e)\n",
    "#print(liste3)\n",
    "#result = merge_clusters(liste2)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masteroppgave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
