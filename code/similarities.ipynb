{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the compuation of the hash-based similarities\n",
    "\n",
    "Containing methods and functionality for computing and measuring similarities from the hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.grid_resolution import plot_grid_res_layers\n",
    "from experiments.disk_resolution import plot_disk_dia_layers\n",
    "from experiments.disk_resolution import plot_disk_numbers\n",
    "\n",
    "import global_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 1 0.20\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 441 and the array at index 1 has size 2500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/cluster/home/andrehva/.conda/envs/masteroppgave/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/andrehva/.conda/envs/masteroppgave/lib/python3.12/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/cluster/home/andrehva/master/code/experiments/grid_resolution.py\", line 74, in _fun_wrapper_corr\n    corr = np.corrcoef(edits, REFERENCE[city.lower()+reference.lower()])[0][1]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/andrehva/.conda/envs/masteroppgave/lib/python3.12/site-packages/numpy/lib/function_base.py\", line 2889, in corrcoef\n    c = cov(x, y, rowvar, dtype=dtype)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/cluster/home/andrehva/.conda/envs/masteroppgave/lib/python3.12/site-packages/numpy/lib/function_base.py\", line 2683, in cov\n    X = np.concatenate((X, y), axis=0)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 441 and the array at index 1 has size 2500\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Test-plot\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plot_grid_res_layers(\u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m], [\u001b[39m0.2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m0.2\u001b[39m], parallell_jobs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, reference\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfrechet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/master/code/experiments/grid_resolution.py:120\u001b[0m, in \u001b[0;36mplot_grid_res_layers\u001b[0;34m(city, layers, resolution, measure, reference, parallell_jobs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_grid_res_layers\u001b[39m(city: \u001b[39mstr\u001b[39m, layers: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], resolution: \u001b[39mlist\u001b[39m[\u001b[39mfloat\u001b[39m], measure: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpy_edp\u001b[39m\u001b[39m\"\u001b[39m, reference: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdtw\u001b[39m\u001b[39m\"\u001b[39m, parallell_jobs: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m ):\n\u001b[1;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Visualises the 'optimal' values for resolution and layers for the grid hashes \u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    Param\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m        Yhe number of parallell jobs that will create the data foundation\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     results \u001b[39m=\u001b[39m _compute_grid_res_layers(city, layers, resolution, measure, reference, parallell_jobs)\n\u001b[1;32m    123\u001b[0m     fig, ax1 \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m8\u001b[39m), dpi\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m)\n\u001b[1;32m    124\u001b[0m     ax2 \u001b[39m=\u001b[39m ax1\u001b[39m.\u001b[39mtwinx()\n",
      "File \u001b[0;32m~/master/code/experiments/grid_resolution.py:90\u001b[0m, in \u001b[0;36m_compute_grid_res_layers\u001b[0;34m(city, layers, resolution, measure, reference, parallell_jobs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL: \u001b[39m\u001b[39m{\u001b[39;00mlay\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(res), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39m#edits = _mirrorDiagonal(MEASURE[measure](hashes)).flatten()\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m#corr = np.corrcoef(edits, REFERENCE[city.lower()+reference.lower()])[0][1]\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m corrs \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mmap(_fun_wrapper_corr, [(city, res, lay, measure, reference) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(parallell_jobs)])\n\u001b[1;32m     91\u001b[0m corr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39marray(corrs) )\n\u001b[1;32m     92\u001b[0m std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(np\u001b[39m.\u001b[39marray(corrs))\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 441 and the array at index 1 has size 2500"
     ]
    }
   ],
   "source": [
    "#Test-plot\n",
    "plot_grid_res_layers(\"porto\", [1,2,3,4], [0.2, 5, 0.2], parallell_jobs=20, reference=\"frechet\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Porto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porto/DTW - Frechet\n",
    "plot_grid_res_layers(\"porto\", [1,2,3,4,5,6,8,10], [0.2, 5, 0.2], parallell_jobs=20, reference=\"frechet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porto/DTW - DTW\n",
    "plot_grid_res_layers(\"porto\", [1,2,3,4,5,6,8,10], [0.2, 5, 0.2], parallell_jobs=20, reference=\"dtw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porto/ED - Frechet\n",
    "plot_grid_res_layers(\"porto\", [1,2,3,4,5,6,8,10], [0.2, 5, 0.2], measure=\"py_ed\", parallell_jobs=20, reference=\"frechet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porto/ED - DTW\n",
    "plot_grid_res_layers(\"porto\", [1,2,3,4,5,6,8,10], [0.2, 5, 0.2], measure=\"py_ed\", parallell_jobs=20, reference=\"dtw\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Porto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disk_dia_layers(\"porto\", [1,2,3,4,5,6], [1, 3, 0.2], measure=\"py_dtw\", reference=\"dtw\", parallell_jobs=20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disk_dia_layers(\"porto\", [1,2,3,4,5,6], [1, 3, 0.2], measure=\"py_dtw\", reference=\"frechet\", parallell_jobs=20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disk_dia_layers(\"porto\", [1,2,3,4,5,6], [1, 3, 0.2], measure=\"py_ed\", reference=\"dtw\", parallell_jobs=20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disk_dia_layers(\"porto\", [1,2,3,4,5,6], [1, 3, 0.2], measure=\"py_ed\", reference=\"frechet\", parallell_jobs=20 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting disk number figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_disk_numbers(\"porto\", 4, 2.2, [10,20,30,40,50,60,70,80,90,100], parallell_jobs=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the hash similarities that will be used for further research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.grid_similarity import generate_grid_hash_similarity\n",
    "from experiments.disk_similarity import generate_disk_hash_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Porto\n",
    "similarities = generate_grid_hash_similarity(\"porto\", 1.6, 5)\n",
    "output_path = \"../code/experiments/similarities/grid_porto.csv\"\n",
    "similarities.to_csv(os.path.abspath(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cant find file ../data/hashed_data/grid/porto/META-1000.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/master/code/utils/metafile_handler.py:96\u001b[0m, in \u001b[0;36mread_meta_file\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path_to_file,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     97\u001b[0m         trajectory_files \u001b[39m=\u001b[39m [ line\u001b[39m.\u001b[39mrstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/hashed_data/grid/porto/META-1000.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Disk Porto\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m similarities \u001b[39m=\u001b[39m generate_disk_hash_similarity(\u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2.2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m60\u001b[39m)\n\u001b[1;32m      3\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../code/experiments/similarities/disk_porto.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m similarities\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(output_path))\n",
      "File \u001b[0;32m~/master/code/experiments/disk_similarity.py:104\u001b[0m, in \u001b[0;36mgenerate_disk_hash_similarity\u001b[0;34m(city, diameter, layers, disks)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generates the full grid hash similarities and saves it as a dataframe \"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m Disk \u001b[39m=\u001b[39m_constructDisk(city, diameter, layers, disks, \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m hashes \u001b[39m=\u001b[39m Disk\u001b[39m.\u001b[39mcompute_dataset_hashes_with_KD_tree_numerical()\n\u001b[1;32m    105\u001b[0m similarities \u001b[39m=\u001b[39m py_dtw_parallell(hashes)\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m similarities\n",
      "File \u001b[0;32m~/master/code/schemes/disk_lsh.py:421\u001b[0m, in \u001b[0;36mDiskLSH.compute_dataset_hashes_with_KD_tree_numerical\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_dataset_hashes_with_KD_tree_numerical\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m]:\n\u001b[1;32m    420\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Same as aboce, but returns the hashes as the disks center coordinates\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     files \u001b[39m=\u001b[39m mfh\u001b[39m.\u001b[39mread_meta_file(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_file)\n\u001b[1;32m    422\u001b[0m     trajectories \u001b[39m=\u001b[39m fh\u001b[39m.\u001b[39mload_trajectory_files(files, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path)\n\u001b[1;32m    424\u001b[0m     \u001b[39m# Beginning to hash trajectories\u001b[39;00m\n",
      "File \u001b[0;32m~/master/code/utils/metafile_handler.py:100\u001b[0m, in \u001b[0;36mread_meta_file\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m     98\u001b[0m         file\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m(\u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCant find file \u001b[39m\u001b[39m{\u001b[39;00mpath_to_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m trajectory_files\n",
      "\u001b[0;31mException\u001b[0m: Cant find file ../data/hashed_data/grid/porto/META-1000.txt"
     ]
    }
   ],
   "source": [
    "# Disk Porto\n",
    "similarities = generate_disk_hash_similarity(\"porto\", 2.2, 4, 60)\n",
    "output_path = \"../code/experiments/similarities/disk_porto.csv\"\n",
    "similarities.to_csv(os.path.abspath(output_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtimes of grid similarity computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.grid_similarity import measure_grid_hash_similarity_computation_time\n",
    "from utils.figure_creator import draw_hash_similarity_runtime, draw_hash_similarity_runtime_logarithmic\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing size 100\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>run_1</th>\n",
       "      <td>1.279667</td>\n",
       "      <td>1.310186</td>\n",
       "      <td>1.387999</td>\n",
       "      <td>1.247891</td>\n",
       "      <td>1.361872</td>\n",
       "      <td>1.341656</td>\n",
       "      <td>1.352935</td>\n",
       "      <td>1.268300</td>\n",
       "      <td>1.323448</td>\n",
       "      <td>1.304557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_2</th>\n",
       "      <td>1.284826</td>\n",
       "      <td>1.305681</td>\n",
       "      <td>1.386045</td>\n",
       "      <td>1.248853</td>\n",
       "      <td>1.359759</td>\n",
       "      <td>1.333218</td>\n",
       "      <td>1.353737</td>\n",
       "      <td>1.267998</td>\n",
       "      <td>1.322378</td>\n",
       "      <td>1.307948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_3</th>\n",
       "      <td>1.282063</td>\n",
       "      <td>1.310134</td>\n",
       "      <td>1.393171</td>\n",
       "      <td>1.248483</td>\n",
       "      <td>1.360821</td>\n",
       "      <td>1.350921</td>\n",
       "      <td>1.352306</td>\n",
       "      <td>1.266009</td>\n",
       "      <td>1.316501</td>\n",
       "      <td>1.306265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_4</th>\n",
       "      <td>1.287920</td>\n",
       "      <td>1.307669</td>\n",
       "      <td>1.401757</td>\n",
       "      <td>1.250610</td>\n",
       "      <td>1.361570</td>\n",
       "      <td>1.333102</td>\n",
       "      <td>1.340792</td>\n",
       "      <td>1.268155</td>\n",
       "      <td>1.319134</td>\n",
       "      <td>1.302410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_5</th>\n",
       "      <td>1.281075</td>\n",
       "      <td>1.307759</td>\n",
       "      <td>1.404818</td>\n",
       "      <td>1.247599</td>\n",
       "      <td>1.360523</td>\n",
       "      <td>1.329841</td>\n",
       "      <td>1.368645</td>\n",
       "      <td>1.268575</td>\n",
       "      <td>1.310928</td>\n",
       "      <td>1.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_6</th>\n",
       "      <td>1.284560</td>\n",
       "      <td>1.308934</td>\n",
       "      <td>1.404397</td>\n",
       "      <td>1.250444</td>\n",
       "      <td>1.357526</td>\n",
       "      <td>1.339741</td>\n",
       "      <td>1.348219</td>\n",
       "      <td>1.263679</td>\n",
       "      <td>1.312871</td>\n",
       "      <td>1.304670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_7</th>\n",
       "      <td>1.280419</td>\n",
       "      <td>1.307436</td>\n",
       "      <td>1.404487</td>\n",
       "      <td>1.246958</td>\n",
       "      <td>1.360346</td>\n",
       "      <td>1.344135</td>\n",
       "      <td>1.342103</td>\n",
       "      <td>1.265313</td>\n",
       "      <td>1.308402</td>\n",
       "      <td>1.306676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_8</th>\n",
       "      <td>1.281778</td>\n",
       "      <td>1.306288</td>\n",
       "      <td>1.396653</td>\n",
       "      <td>1.244236</td>\n",
       "      <td>1.362987</td>\n",
       "      <td>1.333591</td>\n",
       "      <td>1.350049</td>\n",
       "      <td>1.265527</td>\n",
       "      <td>1.311252</td>\n",
       "      <td>1.289270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_9</th>\n",
       "      <td>1.278014</td>\n",
       "      <td>1.305796</td>\n",
       "      <td>1.423040</td>\n",
       "      <td>1.251824</td>\n",
       "      <td>1.351012</td>\n",
       "      <td>1.331233</td>\n",
       "      <td>1.343245</td>\n",
       "      <td>1.267346</td>\n",
       "      <td>1.307228</td>\n",
       "      <td>1.295740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_10</th>\n",
       "      <td>1.282495</td>\n",
       "      <td>1.315276</td>\n",
       "      <td>1.404965</td>\n",
       "      <td>1.248096</td>\n",
       "      <td>1.356245</td>\n",
       "      <td>1.332620</td>\n",
       "      <td>1.351751</td>\n",
       "      <td>1.260989</td>\n",
       "      <td>1.316807</td>\n",
       "      <td>1.289380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             10        20        30        40        50        60        70   \\\n",
       "run_1   1.279667  1.310186  1.387999  1.247891  1.361872  1.341656  1.352935   \n",
       "run_2   1.284826  1.305681  1.386045  1.248853  1.359759  1.333218  1.353737   \n",
       "run_3   1.282063  1.310134  1.393171  1.248483  1.360821  1.350921  1.352306   \n",
       "run_4   1.287920  1.307669  1.401757  1.250610  1.361570  1.333102  1.340792   \n",
       "run_5   1.281075  1.307759  1.404818  1.247599  1.360523  1.329841  1.368645   \n",
       "run_6   1.284560  1.308934  1.404397  1.250444  1.357526  1.339741  1.348219   \n",
       "run_7   1.280419  1.307436  1.404487  1.246958  1.360346  1.344135  1.342103   \n",
       "run_8   1.281778  1.306288  1.396653  1.244236  1.362987  1.333591  1.350049   \n",
       "run_9   1.278014  1.305796  1.423040  1.251824  1.351012  1.331233  1.343245   \n",
       "run_10  1.282495  1.315276  1.404965  1.248096  1.356245  1.332620  1.351751   \n",
       "\n",
       "             80        90        100  \n",
       "run_1   1.268300  1.323448  1.304557  \n",
       "run_2   1.267998  1.322378  1.307948  \n",
       "run_3   1.266009  1.316501  1.306265  \n",
       "run_4   1.268155  1.319134  1.302410  \n",
       "run_5   1.268575  1.310928  1.306390  \n",
       "run_6   1.263679  1.312871  1.304670  \n",
       "run_7   1.265313  1.308402  1.306676  \n",
       "run_8   1.265527  1.311252  1.289270  \n",
       "run_9   1.267346  1.307228  1.295740  \n",
       "run_10  1.260989  1.316807  1.289380  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measuring Grid Porto similarity computation times // idun (1node, 6cores, 128GB): 9min\n",
    "\n",
    "runs = 10\n",
    "data_sets = range(10,110,10)\n",
    "output_folder = \"../code/experiments/timing/\"\n",
    "file_name = \"similarity_runtimes_grid_porto.csv\"\n",
    "\n",
    "df = pd.DataFrame( index=[f\"run_{x+1}\" for x in range(runs)], columns=[x for x in data_sets])\n",
    "\n",
    "for size in data_sets:\n",
    "    print(f\"Computing size {size}\", end=\"\\r\")\n",
    "    execution_times = measure_grid_hash_similarity_computation_time(\"porto\", size, 1.6, 5, \"dtw\", parallell_jobs=10)\n",
    "    df[size] = execution_times\n",
    "\n",
    "df.to_csv(os.path.join(output_folder, file_name))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing subset-100\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset-100</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>run_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.273122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subset-100       100\n",
       "run_1        NaN  1.273122"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SAME AS OVER, BUT FOR 1 RUN\n",
    "\n",
    "# Measuring Grid Porto similarity computation times // idun (1node, 6cores, 128GB): 9min\n",
    "\n",
    "output_folder = \"../code/experiments/timing/\"\n",
    "file_name = f\"similarity_runtimes_grid_porto-{global_variables.CHOSEN_SUBSET_NAME}.csv\"\n",
    "\n",
    "df = pd.DataFrame( index=[\"run_1\"], columns=[global_variables.CHOSEN_SUBSET_NAME])\n",
    "\n",
    "\n",
    "print(f\"Computing {global_variables.CHOSEN_SUBSET_NAME}\", end=\"\\r\")\n",
    "execution_times = measure_grid_hash_similarity_computation_time(\"porto\", global_variables.CHOSEN_SUBSET_SIZE, 1.6, 5, \"dtw\", parallell_jobs=1)\n",
    "df[global_variables.CHOSEN_SUBSET_SIZE] = execution_times\n",
    "\n",
    "df.to_csv(os.path.join(output_folder, file_name))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/home/andrehva/master/code/experiments/timing/similarity_runtimes_disk_porto.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m referencepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m\"\u001b[39m\u001b[39m../code/experiments/timing/similarity_runtimes_true_dtw_porto.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m city \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPorto\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m draw_hash_similarity_runtime_logarithmic(city, grid_path, disk_path, path_to_reference\u001b[39m=\u001b[39mreferencepath)\n",
      "File \u001b[0;32m~/master/code/utils/figure_creator.py:82\u001b[0m, in \u001b[0;36mdraw_hash_similarity_runtime_logarithmic\u001b[0;34m(city, path_grid, path_disk, path_to_reference)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mMethod that draws a figure of the runtime of the hash similarity computation, logarithmic y-scale:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m grid_timing_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_grid, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m disk_timing_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_disk, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     83\u001b[0m reference_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_to_reference, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m path_to_reference \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m grid_mean_timing \u001b[39m=\u001b[39m grid_timing_data\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/masteroppgave/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/home/andrehva/master/code/experiments/timing/similarity_runtimes_disk_porto.csv'"
     ]
    }
   ],
   "source": [
    "# Generating a figure visualising the computation runtime of the hashes over Porto\n",
    "from utils.figure_creator import draw_hash_similarity_runtime_logarithmic\n",
    "import os\n",
    "grid_path = os.path.abspath(\"../code/experiments/timing/similarity_runtimes_grid_porto.csv\")\n",
    "disk_path = os.path.abspath(\"../code/experiments/timing/similarity_runtimes_disk_porto.csv\")\n",
    "referencepath = os.path.abspath(\"../code/experiments/timing/similarity_runtimes_true_dtw_porto.csv\")\n",
    "city = \"Porto\"\n",
    "draw_hash_similarity_runtime_logarithmic(city, grid_path, disk_path, path_to_reference=referencepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtimes of disk similarity computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.disk_similarity import measure_disk_hash_similarity_computation_time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Disk Porto similarity computation times // amalie, lokalt: 105min // idun (1node, 6cores, 128GB): 59min\n",
    "\n",
    "runs = 10\n",
    "data_sets = range(100,1001,100)\n",
    "output_folder = \"../code/experiments/timing/\"\n",
    "file_name = \"similarity_runtimes_disk_porto.csv\"\n",
    "\n",
    "df = pd.DataFrame(index=[f\"run_{x+1}\" for x in range(runs)], columns=[x for x in data_sets])\n",
    "\n",
    "for size in data_sets:\n",
    "    print(f\"Computing size {size}\", end=\"\\r\")\n",
    "    execution_times = measure_disk_hash_similarity_computation_time(\"porto\", size, 2.2, 4, 60,\"kd\", measure=\"dtw\", parallell_jobs=10)\n",
    "    df[size] = execution_times\n",
    "\n",
    "df.to_csv(os.path.join(output_folder, file_name))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing variance from 10 different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.correlation import compute_correlation_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run : 0\n",
      "Run : 1\n",
      "Run : 2\n",
      "Run : 3\n",
      "Run : 4\n",
      "Run : 5\n",
      "Run : 6\n",
      "Run : 7\n",
      "Run : 8\n",
      "Run : 9\n",
      "porto grid : (min, max, avg, std)\n",
      "DTW: 0.5481780170044595 0.5934460853484671 0.5691611920109005 0.014952889640567856\n",
      "FRE: 0.6961329741011646 0.7386663851778994 0.7137623080623746 0.014302696461248916\n"
     ]
    }
   ],
   "source": [
    "compute_correlation_similarity(\"porto\", \"grid\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run : 0\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cant find file ../data/hashed_data/grid/porto/META-1000.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/master/code/utils/metafile_handler.py:96\u001b[0m, in \u001b[0;36mread_meta_file\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path_to_file,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     97\u001b[0m         trajectory_files \u001b[39m=\u001b[39m [ line\u001b[39m.\u001b[39mrstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file ]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/hashed_data/grid/porto/META-1000.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# idun(samme): 24min\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m compute_correlation_similarity(\u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/master/code/experiments/correlation.py:51\u001b[0m, in \u001b[0;36mcompute_correlation_similarity\u001b[0;34m(city, scheme, runs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     hash_sims \u001b[39m=\u001b[39m generate_grid_hash_similarity(\u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1.6\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39melif\u001b[39;00m city\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m scheme\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     hash_sims \u001b[39m=\u001b[39m generate_disk_hash_similarity(\u001b[39m\"\u001b[39m\u001b[39mporto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2.2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m60\u001b[39m)\n\u001b[1;32m     53\u001b[0m h_sims \u001b[39m=\u001b[39m  _mirrorDiagonal(hash_sims)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     54\u001b[0m correlation_dtw\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mcorrcoef(h_sims, true_sims[city][\u001b[39m\"\u001b[39m\u001b[39mdtw\u001b[39m\u001b[39m\"\u001b[39m])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/master/code/experiments/disk_similarity.py:104\u001b[0m, in \u001b[0;36mgenerate_disk_hash_similarity\u001b[0;34m(city, diameter, layers, disks)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generates the full grid hash similarities and saves it as a dataframe \"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m Disk \u001b[39m=\u001b[39m_constructDisk(city, diameter, layers, disks, \u001b[39m1000\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m hashes \u001b[39m=\u001b[39m Disk\u001b[39m.\u001b[39mcompute_dataset_hashes_with_KD_tree_numerical()\n\u001b[1;32m    105\u001b[0m similarities \u001b[39m=\u001b[39m py_dtw_parallell(hashes)\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m similarities\n",
      "File \u001b[0;32m~/master/code/schemes/disk_lsh.py:421\u001b[0m, in \u001b[0;36mDiskLSH.compute_dataset_hashes_with_KD_tree_numerical\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_dataset_hashes_with_KD_tree_numerical\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m]:\n\u001b[1;32m    420\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Same as aboce, but returns the hashes as the disks center coordinates\"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     files \u001b[39m=\u001b[39m mfh\u001b[39m.\u001b[39mread_meta_file(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_file)\n\u001b[1;32m    422\u001b[0m     trajectories \u001b[39m=\u001b[39m fh\u001b[39m.\u001b[39mload_trajectory_files(files, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path)\n\u001b[1;32m    424\u001b[0m     \u001b[39m# Beginning to hash trajectories\u001b[39;00m\n",
      "File \u001b[0;32m~/master/code/utils/metafile_handler.py:100\u001b[0m, in \u001b[0;36mread_meta_file\u001b[0;34m(path_to_file)\u001b[0m\n\u001b[1;32m     98\u001b[0m         file\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m(\u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCant find file \u001b[39m\u001b[39m{\u001b[39;00mpath_to_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m trajectory_files\n",
      "\u001b[0;31mException\u001b[0m: Cant find file ../data/hashed_data/grid/porto/META-1000.txt"
     ]
    }
   ],
   "source": [
    "# idun(samme): 24min\n",
    "compute_correlation_similarity(\"porto\", \"disk\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a8cebea5680a70624bfb085291ac11ac025ad85fe214511868677846ba38b53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
